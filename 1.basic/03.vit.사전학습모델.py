# huggingface transformer vit
import torch
import torch.nn.functional as F
from torchvision import transforms
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt
import requests
from io import BytesIO
import os, json
from glob import glob

"""
ViT ì‚¬ì „í•™ìŠµ ëª¨ë¸ ë¹„êµ: Hugging Face vs timm

=== ì „ì²´ ìŠ¤í† ë¦¬ ===
ê°™ì€ ViT ëª¨ë¸ì´ì§€ë§Œ ë‘ ê°€ì§€ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. Hugging Face Transformers
   - ìì—°ì–´ ì²˜ë¦¬(NLP)ë¡œ ìœ ëª…í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬
   - ì‚¬ìš©í•˜ê¸° ì‰½ê³ , ì „ì²˜ë¦¬ê°€ ìë™í™”ë¨
   - ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œê°€ ì„¸íŠ¸ë¡œ ì œê³µ

2. timm (PyTorch Image Models)
   - ì´ë¯¸ì§€ ëª¨ë¸ ì „ë¬¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
   - ë‹¤ì–‘í•œ ëª¨ë¸ ë³€í˜• ì œê³µ
   - ë” ë§ì€ ì»¤ìŠ¤í„°ë§ˆì´ì§• ê°€ëŠ¥

ì˜¤ëŠ˜ì€ ê°™ì€ ì´ë¯¸ì§€ë¡œ ë‘ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ë¹„êµí•´ë´…ë‹ˆë‹¤!
"""

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False


def use_huggingface_vit():
    '''
    Hugging Face Transformersë¡œ ViT ëª¨ë¸ ë¡œë“œ
    
    === ìŠ¤í† ë¦¬ ===
    Hugging FaceëŠ” ë§ˆì¹˜ "ì˜¬ì¸ì› íŒ¨í‚¤ì§€"ê°™ìŠµë‹ˆë‹¤.
    ëª¨ë¸ ë‹¤ìš´ë¡œë“œ â†’ ì „ì²˜ë¦¬ ì„¤ì • â†’ í´ë˜ìŠ¤ ë¼ë²¨ê¹Œì§€
    ëª¨ë“  ê²ƒì´ ìë™ìœ¼ë¡œ ì¤€ë¹„ë©ë‹ˆë‹¤!
    
    ë§ˆì¹˜ ì¡°ë¦¬ ì™„ë£Œëœ ë°€í‚¤íŠ¸ë¥¼ ë°›ëŠ” ê²ƒì²˜ëŸ¼,
    ë°”ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ëª¨ë“  ê²Œ ì¤€ë¹„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    '''
    try:
        from transformers import ViTForImageClassification, ViTImageProcessor
        
        print("=" * 60)
        print("[Hugging Face ViT ëª¨ë¸ ë¡œë“œ]")
        print("=" * 60)
        
        # ============================================================
        # ëª¨ë¸ê³¼ í”„ë¡œì„¸ì„œ ë¡œë“œ
        # ============================================================
        # model_name: Hugging Face Hubì— ìˆëŠ” ëª¨ë¸ì˜ ì£¼ì†Œ
        # "google/vit-base-patch16-224"ì˜ ì˜ë¯¸:
        #   - google: Googleì´ í•™ìŠµì‹œí‚¨ ëª¨ë¸
        #   - vit-base: ViT Base í¬ê¸° (86M íŒŒë¼ë¯¸í„°)
        #   - patch16: 16Ã—16 íŒ¨ì¹˜ ì‚¬ìš©
        #   - 224: 224Ã—224 ì´ë¯¸ì§€ ì…ë ¥
        #
        # ì´ ì½”ë“œë¥¼ ì²˜ìŒ ì‹¤í–‰í•˜ë©´:
        # 1. ì¸í„°ë„·ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì•½ 350MB)
        # 2. ë¡œì»¬ ìºì‹œì— ì €ì¥ (ë‹¤ìŒë¶€í„°ëŠ” ë¹ ë¦„)
        # 3. ë©”ëª¨ë¦¬ì— ë¡œë“œ
        model_name = 'google/vit-base-patch16-224'
        print(f"\nëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤‘: {model_name}")
        print("(ì²˜ìŒ ì‹¤í–‰ ì‹œ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)")
        
        # ViTImageProcessor: ì´ë¯¸ì§€ ì „ì²˜ë¦¬ ë‹´ë‹¹
        # - ìë™ìœ¼ë¡œ 224Ã—224 ë¦¬ì‚¬ì´ì¦ˆ
        # - ImageNet í†µê³„ë¡œ ì •ê·œí™”
        # - í…ì„œ ë³€í™˜
        # ìš°ë¦¬ê°€ í•  ì¼: ì´ë¯¸ì§€ë§Œ ë„£ìœ¼ë©´ ë¨!
        processor = ViTImageProcessor.from_pretrained(model_name)
        
        # ViTForImageClassification: ë¶„ë¥˜ ëª¨ë¸
        # - ì´ë¯¸ì§€ ì…ë ¥ â†’ 1000ê°œ í´ë˜ìŠ¤ í™•ë¥  ì¶œë ¥
        # - ImageNet í´ë˜ìŠ¤ ë¼ë²¨ ë‚´ì¥
        model = ViTForImageClassification.from_pretrained(model_name)
        
        # eval() ëª¨ë“œ:
        # - í•™ìŠµ ëª¨ë“œ OFF
        # - Dropout ë¹„í™œì„±í™”
        # - BatchNorm ê³ ì •
        # ì¶”ë¡ í•  ë•ŒëŠ” í•­ìƒ eval() í•„ìš”!
        model.eval()
        
        # ============================================================
        # ëª¨ë¸ ìƒì„¸ ì •ë³´ ì¶œë ¥
        # ============================================================
        print(f'\n[ëª¨ë¸ ì •ë³´]')
        
        # íŒŒë¼ë¯¸í„° ìˆ˜: ëª¨ë¸ì˜ "í•™ìŠµ ê°€ëŠ¥í•œ ìˆ«ìë“¤"ì˜ ê°œìˆ˜
        # ViT-BaseëŠ” ì•½ 86M (8ì²œ6ë°±ë§Œ) ê°œ
        total_params = sum(p.numel() for p in model.parameters())
        print(f'  íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,} ({total_params/1e6:.1f}M)')
        
        # config: ëª¨ë¸ì˜ ì„¤ì • ì •ë³´ (ì„¤ê³„ë„)
        print(f'  ì…ë ¥ ì±„ë„ ìˆ˜: {model.config.num_channels}')  # RGB = 3
        print(f'  ì´ë¯¸ì§€ í¬ê¸°: {model.config.image_size}Ã—{model.config.image_size}')  # 224Ã—224
        print(f'  íŒ¨ì¹˜ í¬ê¸°: {model.config.patch_size}Ã—{model.config.patch_size}')  # 16Ã—16
        print(f'  íˆë“  í¬ê¸°: {model.config.hidden_size}')  # 768 (ì„ë² ë”© ì°¨ì›)
        print(f'  ë ˆì´ì–´ ìˆ˜: {model.config.num_hidden_layers}')  # 12ê°œ Transformer ë¸”ë¡
        print(f'  ì–´í…ì…˜ í—¤ë“œ ìˆ˜: {model.config.num_attention_heads}')  # 12ê°œ í—¤ë“œ
        print(f'  í´ë˜ìŠ¤ ìˆ˜: {len(model.config.id2label)}')  # 1000ê°œ (ImageNet)
        
        # id2label: ìˆ«ì â†’ í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
        # ì˜ˆ: 281 â†’ "tabby cat"
        #     207 â†’ "golden retriever"
        
        return model, processor
        
    except Exception as e:
        print(f'âœ— Hugging Face ViT ë¡œë“œ ì‹¤íŒ¨: {e}')
        return None, None


def use_timm_vit():
    '''
    timm ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ViT ëª¨ë¸ ë¡œë“œ
    
    === ìŠ¤í† ë¦¬ ===
    timmì€ "ì´ë¯¸ì§€ ëª¨ë¸ ë°±í™”ì "ì…ë‹ˆë‹¤.
    ìˆ˜ë°± ê°€ì§€ ì‚¬ì „í•™ìŠµ ëª¨ë¸ì„ ì œê³µí•˜ê³ ,
    ë” ì„¸ë°€í•œ ì„¤ì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.
    
    Hugging Faceê°€ "ì‰¬ìš´ ìë™í™”"ë¼ë©´,
    timmì€ "ì „ë¬¸ê°€ìš© ë„êµ¬"ì— ê°€ê¹ìŠµë‹ˆë‹¤.
    '''
    try:
        import timm
        
        print("\n" + "=" * 60)
        print("[timm ViT ëª¨ë¸ ë¡œë“œ]")
        print("=" * 60)
        
        # ============================================================
        # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í™•ì¸
        # ============================================================
        # timm.list_models(): timmì—ì„œ ì œê³µí•˜ëŠ” ëª¨ë“  ëª¨ë¸ ê²€ìƒ‰
        # 'vit*': ì´ë¦„ì— 'vit'ê°€ ë“¤ì–´ê°„ ëª¨ë¸ë§Œ
        # pretrained=True: ì‚¬ì „í•™ìŠµëœ ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ê²ƒë§Œ
        #
        # ViT ë³€í˜•ë“¤:
        # - vit_tiny: ì‘ì€ ëª¨ë¸ (5M)
        # - vit_small: ì¤‘ê°„ ëª¨ë¸ (22M)
        # - vit_base: ê¸°ë³¸ ëª¨ë¸ (86M) â† ìš°ë¦¬ê°€ ì‚¬ìš©
        # - vit_large: í° ëª¨ë¸ (307M)
        # - vit_huge: ë§¤ìš° í° ëª¨ë¸ (632M)
        vit_models = timm.list_models('vit*', pretrained=True)
        print(f'\nì‚¬ìš© ê°€ëŠ¥í•œ ViT ëª¨ë¸ (ì¼ë¶€):')
        for model_name in vit_models[:5]:  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
            print(f'  - {model_name}')
        print(f'  ... (ì´ {len(vit_models)}ê°œ ëª¨ë¸)')
        
        # ============================================================
        # ëª¨ë¸ ìƒì„±
        # ============================================================
        # timm.create_model():
        # - ëª¨ë¸ ì•„í‚¤í…ì²˜ ìƒì„±
        # - pretrained=True: í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë‹¤ìš´ë¡œë“œ
        # 
        # 'vit_base_patch16_224':
        # - base: 86M íŒŒë¼ë¯¸í„°
        # - patch16: 16Ã—16 íŒ¨ì¹˜
        # - 224: 224Ã—224 ì…ë ¥
        print(f'\nëª¨ë¸ ë¡œë“œ ì¤‘: vit_base_patch16_224')
        model = timm.create_model('vit_base_patch16_224', pretrained=True)
        model.eval()
        
        print(f"âœ“ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        
        # ============================================================
        # ë°ì´í„° ì „ì²˜ë¦¬ ì„¤ì •
        # ============================================================
        # timmì€ ì „ì²˜ë¦¬ë¥¼ ì§ì ‘ ì„¤ì •í•´ì•¼ í•¨
        # (Hugging FaceëŠ” ìë™ì´ì—ˆìŒ)
        #
        # data_config: ëª¨ë¸ì— ë§ëŠ” ì „ì²˜ë¦¬ ì„¤ì •
        # - input_size: ì…ë ¥ í¬ê¸°
        # - mean: ì •ê·œí™” í‰ê· 
        # - std: ì •ê·œí™” í‘œì¤€í¸ì°¨
        # - interpolation: ë¦¬ì‚¬ì´ì¦ˆ ë°©ë²•
        data_config = timm.data.resolve_model_data_config(model)
        
        # create_transform: ì‹¤ì œ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ìƒì„±
        # is_training=False: í‰ê°€ ëª¨ë“œ (ì¦ê°• ì—†ìŒ)
        transform = timm.data.create_transform(**data_config, is_training=False)
        
        return model, transform
        
    except Exception as e:
        print(f'âœ— timm ViT ë¡œë“œ ì‹¤íŒ¨: {e}')
        print('ğŸ’¡ ì„¤ì¹˜ ë°©ë²•: pip install timm')
        return None, None


def classify_image_hf(model, processor, image):
    """
    Hugging Face ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜
    
    === ìŠ¤í† ë¦¬ ===
    1. ì´ë¯¸ì§€ë¥¼ processorì— ë„£ìœ¼ë©´ ìë™ ì „ì²˜ë¦¬
    2. ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ì…ë ¥
    3. 1000ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ ì ìˆ˜(logits) ì¶œë ¥
    4. Softmaxë¡œ í™•ë¥  ë³€í™˜
    5. ê°€ì¥ ë†’ì€ í™•ë¥  Top-5 ì¶œë ¥
    
    ë§ˆì¹˜ ì‚¬ì§„ì„ ì°ì–´ì„œ "ì´ê²Œ ë­ì•¼?"ë¼ê³  ë¬¼ì–´ë³´ë©´
    "85% í™•ë¥ ë¡œ ê³ ì–‘ì´, 10% í™•ë¥ ë¡œ í˜¸ë‘ì´..." ë‹µí•˜ëŠ” ê²ƒ!
    """
    
    if model is None:
        print("  âœ— ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    # ============================================================
    # [1ë‹¨ê³„] ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    # ============================================================
    # processor(images=image, return_tensors="pt"):
    # - images=image: PIL Image ê°ì²´ ì…ë ¥
    # - return_tensors="pt": PyTorch í…ì„œë¡œ ë°˜í™˜
    #
    # ë‚´ë¶€ì—ì„œ ì¼ì–´ë‚˜ëŠ” ì¼:
    # 1. ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (224Ã—224)
    # 2. ì •ê·œí™”: (pixel - mean) / std
    # 3. [0, 255] â†’ [-2, 2] ë²”ìœ„ë¡œ ë³€í™˜
    # 4. (H, W, C) â†’ (C, H, W) ì°¨ì› ë³€ê²½
    # 5. ë°°ì¹˜ ì°¨ì› ì¶”ê°€ [1, 3, 224, 224]
    inputs = processor(images=image, return_tensors="pt")
    
    print(f"\n[ì „ì²˜ë¦¬ëœ ì…ë ¥]")
    print(f"  pixel_values shape: {inputs['pixel_values'].shape}")  # [1, 3, 224, 224]
    # 1: ë°°ì¹˜ í¬ê¸° (ì´ë¯¸ì§€ 1ì¥)
    # 3: RGB ì±„ë„
    # 224Ã—224: ì´ë¯¸ì§€ í¬ê¸°
    
    # ============================================================
    # [2ë‹¨ê³„] ëª¨ë¸ ì¶”ë¡ 
    # ============================================================
    # torch.no_grad(): gradient ê³„ì‚° ì•ˆ í•¨ (ë©”ëª¨ë¦¬ ì ˆì•½)
    # ì¶”ë¡ í•  ë•ŒëŠ” ì—­ì „íŒŒê°€ í•„ìš” ì—†ìœ¼ë¯€ë¡œ!
    with torch.no_grad():
        # model(**inputs): ë”•ì…”ë„ˆë¦¬ ì–¸íŒ¨í‚¹
        # inputs = {'pixel_values': tensor}
        # â†’ model(pixel_values=tensor)
        outputs = model(**inputs)
    
    # logits: ì›ì‹œ ì¶œë ¥ ì ìˆ˜ (í™•ë¥  ì•„ë‹˜!)
    # shape: [1, 1000]
    # - 1: ë°°ì¹˜
    # - 1000: ImageNet í´ë˜ìŠ¤ ê°œìˆ˜
    #
    # logitsì˜ ì˜ë¯¸:
    # - ë†’ì€ ê°’: ëª¨ë¸ì´ í™•ì‹ 
    # - ë‚®ì€ ê°’: ëª¨ë¸ì´ ì˜ì‹¬
    # ì˜ˆ: [2.5, -1.3, 0.8, ...]
    logits = outputs.logits
    
    print(f"\n[ëª¨ë¸ ì¶œë ¥]")
    print(f"  logits shape: {logits.shape}")  # [1, 1000]
    
    # ============================================================
    # [3ë‹¨ê³„] í™•ë¥ ë¡œ ë³€í™˜
    # ============================================================
    # F.softmax(): logits â†’ í™•ë¥ 
    # dim=-1: ë§ˆì§€ë§‰ ì°¨ì›(1000ê°œ í´ë˜ìŠ¤)ì— ëŒ€í•´
    #
    # Softmax ê³µì‹:
    # p_i = exp(logit_i) / sum(exp(logit_j))
    #
    # ê²°ê³¼:
    # - ëª¨ë“  í™•ë¥ ì˜ í•© = 1.0
    # - ê° ê°’ì€ 0~1 ì‚¬ì´
    # ì˜ˆ: [0.85, 0.05, 0.03, ...]
    probs = F.softmax(logits, dim=-1)
    
    # ============================================================
    # [4ë‹¨ê³„] Top-5 ì¶”ì¶œ
    # ============================================================
    # torch.topk(probs, 5):
    # - ê°€ì¥ ë†’ì€ í™•ë¥  5ê°œë¥¼ ì°¾ê¸°
    # - ë°˜í™˜: (í™•ë¥ ê°’, ì¸ë±ìŠ¤)
    #
    # ì˜ˆ:
    # top5_probs = [0.85, 0.08, 0.03, 0.02, 0.01]
    # top5_indices = [281, 282, 283, 207, 285]
    top5_probs, top5_indices = torch.topk(probs, 5)
    
    # ============================================================
    # [5ë‹¨ê³„] ê²°ê³¼ ì¶œë ¥
    # ============================================================
    print(f"\n[Top-5 ì˜ˆì¸¡ ê²°ê³¼]")
    for i, (prob, idx) in enumerate(zip(top5_probs[0], top5_indices[0])):
        # id2label: ì¸ë±ìŠ¤ â†’ í´ë˜ìŠ¤ ì´ë¦„ ë³€í™˜
        # 281 â†’ "tabby cat"
        # 207 â†’ "golden retriever"
        label = model.config.id2label[idx.item()]
        
        # ì¶œë ¥ í˜•ì‹:
        # 1. tabby cat              : 85.23%
        # 2. tiger cat              :  8.14%
        print(f"  {i+1}. {label:30s}: {prob.item()*100:6.2f}%")
    
    return top5_probs[0], top5_indices[0]


def classify_image_timm(model, transform, image):
    """
    timm ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜
    
    === ìŠ¤í† ë¦¬ ===
    Hugging Faceì™€ ê±°ì˜ ë¹„ìŠ·í•˜ì§€ë§Œ,
    ì „ì²˜ë¦¬ë¥¼ ì§ì ‘ transformìœ¼ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.
    
    ë˜í•œ í´ë˜ìŠ¤ ì´ë¦„ì„ ì¸í„°ë„·ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨
    (timmì€ ëª¨ë¸ë§Œ ì œê³µ, ë¼ë²¨ì€ ë³„ë„)
    """
    
    if model is None:
        print("  âœ— ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return None
    
    # ============================================================
    # [1ë‹¨ê³„] ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    # ============================================================
    # transform(image): PIL Image â†’ í…ì„œ
    # ë‚´ë¶€ ì²˜ë¦¬:
    # 1. Resize & CenterCrop
    # 2. ToTensor: [0, 255] â†’ [0, 1]
    # 3. Normalize: (x - mean) / std
    #
    # ê²°ê³¼: [3, 224, 224]
    img_tensor = transform(image)
    
    # unsqueeze(0): ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    # [3, 224, 224] â†’ [1, 3, 224, 224]
    img_tensor = img_tensor.unsqueeze(0)
    
    print(f"\n[ì „ì²˜ë¦¬ëœ ì…ë ¥]")
    print(f"  tensor shape: {img_tensor.shape}")
    
    # ============================================================
    # [2ë‹¨ê³„] ëª¨ë¸ ì¶”ë¡ 
    # ============================================================
    with torch.no_grad():
        # timm ëª¨ë¸ì€ ì§ì ‘ í…ì„œë¥¼ ë°›ìŒ
        # (Hugging FaceëŠ” ë”•ì…”ë„ˆë¦¬ì˜€ìŒ)
        outputs = model(img_tensor)
    
    # outputs: [1, 1000] logits
    # Hugging Faceì™€ ë™ì¼í•œ í˜•ì‹
    print(f"\n[ëª¨ë¸ ì¶œë ¥]")
    print(f"  outputs shape: {outputs.shape}")
    
    # ============================================================
    # [3ë‹¨ê³„] í™•ë¥  ë³€í™˜ & Top-5
    # ============================================================
    probs = F.softmax(outputs, dim=-1)
    top5_probs, top5_indices = torch.topk(probs, 5)
    
    # ============================================================
    # [4ë‹¨ê³„] í´ë˜ìŠ¤ ë¼ë²¨ ê°€ì ¸ì˜¤ê¸°
    # ============================================================
    # timmì€ í´ë˜ìŠ¤ ì´ë¦„ì„ ì œê³µí•˜ì§€ ì•ŠìŒ!
    # ImageNet ë¼ë²¨ì„ ì¸í„°ë„·ì—ì„œ ë‹¤ìš´ë¡œë“œí•´ì•¼ í•¨
    try:
        # GitHubì— ìˆëŠ” ImageNet í´ë˜ìŠ¤ íŒŒì¼
        url = "https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt"
        response = requests.get(url, timeout=10)
        
        # 1000ì¤„ì˜ í…ìŠ¤íŠ¸ íŒŒì¼
        # ê° ì¤„ = í´ë˜ìŠ¤ ì´ë¦„
        # ì˜ˆ:
        # 0: tench
        # 1: goldfish
        # ...
        # 281: tabby cat
        categories = [s.strip() for s in response.text.splitlines()]
        
        print(f"\n[Top-5 ì˜ˆì¸¡ ê²°ê³¼]")
        for i, (prob, idx) in enumerate(zip(top5_probs[0], top5_indices[0])):
            # categories[ì¸ë±ìŠ¤] = í´ë˜ìŠ¤ ì´ë¦„
            label = categories[idx.item()] if idx.item() < len(categories) else f"class_{idx.item()}"
            print(f"  {i+1}. {label:30s}: {prob.item()*100:6.2f}%")
            
    except Exception as e:
        # ì¸í„°ë„· ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¸ë±ìŠ¤ë§Œ ì¶œë ¥
        print(f"\n[Top-5 ì˜ˆì¸¡ ê²°ê³¼ (ì¸ë±ìŠ¤)]")
        for i, (prob, idx) in enumerate(zip(top5_probs[0], top5_indices[0])):
            print(f"  {i+1}. class_{idx.item():4d}: {prob.item()*100:6.2f}%")
    
    return top5_probs[0], top5_indices[0]


if __name__ == '__main__':
    os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
    
    print("\n" + "=" * 70)
    print(" " * 20 + "ViT ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸")
    print(" " * 15 + "Hugging Face vs timm")
    print("=" * 70)
    
    # ============================================================
    # [1ë‹¨ê³„] ëª¨ë¸ ë¡œë“œ
    # ============================================================
    # ë‘ ê°€ì§€ ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ê°™ì€ ViT-Base ëª¨ë¸ ë¡œë“œ
    # - Hugging Face: ìë™í™”, ì‰¬ì›€
    # - timm: ì „ë¬¸ê°€ìš©, ìœ ì—°í•¨
    print("\n[1ë‹¨ê³„] ëª¨ë¸ ë¡œë“œ")
    hf_model, hf_processor = use_huggingface_vit()
    timm_model, timm_transform = use_timm_vit()
    
    # ============================================================
    # [2ë‹¨ê³„] í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì¤€ë¹„
    # ============================================================
    # glob: ë””ë ‰í† ë¦¬ì—ì„œ íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ ì°¾ê¸°
    # '*.jpg': jpg í™•ì¥ìë¥¼ ê°€ì§„ ëª¨ë“  íŒŒì¼
    file_paths = r'C:\Users\sally\OneDrive\ë¬¸ì„œ\GitHub\MultiModal\1.basic\download_img'
    files = glob(os.path.join(file_paths, '*.jpg'))
    
    if not files:
        print(f"\nâœ— ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_paths}")
    else:
        print(f"\nâœ“ ì´ {len(files)}ê°œ ì´ë¯¸ì§€ ë°œê²¬")
        for f in files:
            print(f"  - {os.path.basename(f)}")
        
        # ============================================================
        # [3ë‹¨ê³„] Hugging Face ëª¨ë¸ë¡œ ì¶”ë¡ 
        # ============================================================
        print("\n" + "=" * 70)
        print("[2ë‹¨ê³„] Hugging Face ëª¨ë¸ë¡œ ì¶”ë¡ ")
        print("=" * 70)
        
        for idx, file in enumerate(files):
            print("\n" + "-" * 60)
            print(f"ğŸ“· ì´ë¯¸ì§€ {idx+1}/{len(files)}: {os.path.basename(file)}")
            print("-" * 60)
            
            # PIL Imageë¡œ ë¡œë“œ
            # .convert('RGB'): ëª¨ë“  ì´ë¯¸ì§€ë¥¼ RGBë¡œ í†µì¼
            # (RGBA, í‘ë°± ë“± ë‹¤ì–‘í•œ í˜•ì‹ ëŒ€ì‘)
            test_img = Image.open(file).convert('RGB')
            print(f"ì´ë¯¸ì§€ í¬ê¸°: {test_img.size}")
            
            # Hugging Face ëª¨ë¸ë¡œ ì˜ˆì¸¡
            if hf_model is not None:
                classify_image_hf(hf_model, hf_processor, test_img)
        
        # ============================================================
        # [4ë‹¨ê³„] timm ëª¨ë¸ë¡œ ì¶”ë¡ 
        # ============================================================
        if timm_model is not None:
            print("\n" + "=" * 70)
            print("[3ë‹¨ê³„] timm ëª¨ë¸ë¡œ ì¶”ë¡ ")
            print("=" * 70)
            
            for idx, file in enumerate(files):
                print("\n" + "-" * 60)
                print(f"ğŸ“· ì´ë¯¸ì§€ {idx+1}/{len(files)}: {os.path.basename(file)}")
                print("-" * 60)
                
                test_img = Image.open(file).convert('RGB')
                print(f"ì´ë¯¸ì§€ í¬ê¸°: {test_img.size}")
                
                # timm ëª¨ë¸ë¡œ ì˜ˆì¸¡
                classify_image_timm(timm_model, timm_transform, test_img)
    
    print("\n" + "=" * 70)
    print("âœ“ ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    print("=" * 70)
    print("\nğŸ’¡ ê²°ê³¼ ë¹„êµ:")
    print("  - Hugging Face: ìë™ ì „ì²˜ë¦¬, í´ë˜ìŠ¤ ë¼ë²¨ ë‚´ì¥")
    print("  - timm: ìˆ˜ë™ ì „ì²˜ë¦¬, ë‹¤ì–‘í•œ ëª¨ë¸ ì„ íƒ")
    print("  - ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ëŠ” ê±°ì˜ ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤!")